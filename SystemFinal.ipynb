{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/V0IG3R/Chakravyuh/blob/main/SystemFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4tsZJ_9WO0xI",
        "outputId": "9ebe3323-1920-4b50-cbf9-b08a2cc24025",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pushbullet.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTK05qfNhQFr",
        "outputId": "367b89ff-4bdc-4300-d240-97e7e097cffa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pushbullet.py in /usr/local/lib/python3.9/dist-packages (0.12.0)\n",
            "Requirement already satisfied: requests>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pushbullet.py) (2.27.1)\n",
            "Requirement already satisfied: websocket-client>=0.53.0 in /usr/local/lib/python3.9/dist-packages (from pushbullet.py) (1.5.1)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.9/dist-packages (from pushbullet.py) (0.4.27)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=1.0.0->pushbullet.py) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=1.0.0->pushbullet.py) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=1.0.0->pushbullet.py) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=1.0.0->pushbullet.py) (1.26.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8xqGPZB_UQYt"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pickle\n",
        "import cv2\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from PIL import Image\n",
        "from PIL import ImageEnhance\n",
        "import os\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib.patches import Circle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0VpA9Hc_Us7O"
      },
      "outputs": [],
      "source": [
        "def getTime():\n",
        "  IST = pytz.timezone('Asia/Kolkata')\n",
        "  timeNow = datetime.now(IST)\n",
        "  return timeNow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "B4aoYs4QYpQl"
      },
      "outputs": [],
      "source": [
        "def imgenhance():\n",
        "  image1 = Image.open('savedImage.jpg')\n",
        "  curr_bri = ImageEnhance.Sharpness(image1)\n",
        "  new_bri = 1.3\n",
        "  img_brightened = curr_bri.enhance(new_bri)\n",
        "  im1 = img_brightened.save(\"bright.jpg\")\n",
        "\n",
        "  image2 = Image.open('bright.jpg')\n",
        "  curr_col = ImageEnhance.Color(image2)\n",
        "  new_col = 1.5\n",
        "  img_col = curr_col.enhance(new_col)\n",
        "  im2 = img_col.save(\"finalImage.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pushbullet import Pushbullet\n",
        "\n",
        "def push_vio():\n",
        "  pb = Pushbullet('o.eSNeRC0oCOyvtqmuXWhEul6oudFTAyec')\n",
        "\n",
        "  devices = ['ujyalL8mfoisjAZYWtbUPY']\n",
        "  message = 'Alert! Suspicious activity detected\\nClick on the following link'\n",
        "  for device in devices:  \n",
        "    # push = device.push_note('Notification', message)\n",
        "    push = pb.push_link(message,'https://drive.google.com/file/d/1pQaHpPuMjTuiRFQIDvIlSeDzXqb0q2EJ/view?usp=share_link')\n",
        "\n",
        "def push_mob():\n",
        "  pb = Pushbullet('o.eSNeRC0oCOyvtqmuXWhEul6oudFTAyec')\n",
        "\n",
        "  devices = ['ujyalL8mfoisjAZYWtbUPY']\n",
        "  message = 'Alert! Suspicious activity detected\\nClick on the following link'\n",
        "  for device in devices:  \n",
        "    # push = device.push_note('Notification', message)\n",
        "    push = pb.push_link(message,'https://drive.google.com/file/d/15hB4Zhsjh2FWMJEf3nQ-YPF7q9nini7c/view?usp=share_link')\n",
        "\n",
        "def push_wep():\n",
        "  pb = Pushbullet('o.eSNeRC0oCOyvtqmuXWhEul6oudFTAyec')\n",
        "\n",
        "  devices = ['ujyalL8mfoisjAZYWtbUPY']\n",
        "  message = 'Alert! Suspicious activity detected\\nClick on the following link'\n",
        "  for device in devices:  \n",
        "    # push = device.push_note('Notification', message)\n",
        "    push = pb.push_link(message,'https://drive.google.com/file/d/1mtSvMDhCjYkgJ2SK4M0E914X7r16gTgO/view?usp=share_link')"
      ],
      "metadata": {
        "id": "4ciOgY0HhMh7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "3AIXLE-tUwbw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import pickle\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import time \n",
        "from keras.models import load_model\n",
        "from collections import deque\n",
        "\n",
        "def detectViolence(video, limit=None):\n",
        "        trueCount = 0\n",
        "        imageSaved = 0\n",
        "        filename = 'savedImage.jpg'\n",
        "        my_image = 'finalImage.jpg'\n",
        "        face_image = 'faces.png'\n",
        "        sendAlert = 0\n",
        "        location = \"A\"\n",
        "\n",
        "        print(\"Loading model ...\")\n",
        "        model = load_model('/content/drive/MyDrive/WVM/Violence/modelnew.h5')\n",
        "        Q = deque(maxlen=128)\n",
        "        vs = cv2.VideoCapture(video)\n",
        "        writer = None\n",
        "        (W, H) = (None, None)\n",
        "        count = 0     \n",
        "        while True:\n",
        "            (grabbed, frame) = vs.read()\n",
        "\n",
        "            \n",
        "            if not grabbed:\n",
        "                break\n",
        "            \n",
        "            # if the frame dimensions are empty, grab them\n",
        "            if W is None or H is None:\n",
        "                (H, W) = frame.shape[:2]\n",
        "\n",
        "            # clone the output frame, then convert it from BGR to RGB\n",
        "            # ordering, resize the frame to a fixed 128x128, and then\n",
        "            # perform mean subtraction\n",
        "\n",
        "            \n",
        "            output = frame.copy()\n",
        "           \n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, (128, 128)).astype(\"float32\")\n",
        "            frame = frame.reshape(128, 128, 3) / 255\n",
        "\n",
        "            # make predictions on the frame and then update the predictions\n",
        "            # queue\n",
        "            preds = model.predict(np.expand_dims(frame, axis=0))[0]\n",
        "#             print(\"preds\",preds)\n",
        "            Q.append(preds)\n",
        "\n",
        "            # perform prediction averaging over the current history of\n",
        "            # previous predictions\n",
        "            results = np.array(Q).mean(axis=0)\n",
        "            i = (preds > 0.50)[0]\n",
        "            label = i\n",
        "\n",
        "            text_color = (0, 255, 0) # default : green\n",
        "\n",
        "            if label: # Violence prob\n",
        "                text_color = (0, 0, 255) # red\n",
        "                trueCount = trueCount + 1\n",
        "\n",
        "            else:\n",
        "                text_color = (0, 255, 0)\n",
        "\n",
        "            text = \"Violence: {}\".format(label)\n",
        "            FONT = cv2.FONT_HERSHEY_SIMPLEX \n",
        "\n",
        "            cv2.putText(output, text, (35, 50), FONT,1.25, text_color, 3) \n",
        "\n",
        "            # check if the video writer is None\n",
        "            if writer is None:\n",
        "                # initialize our video writer\n",
        "                fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "                writer = cv2.VideoWriter(\"recordedVideo.avi\", fourcc, 30,(W, H), True)\n",
        "                push_vio()\n",
        "\n",
        "            # write the output frame to disk\n",
        "            writer.write(output)\n",
        "\n",
        "            if(trueCount == 40):\n",
        "              if(imageSaved == 0):\n",
        "                if(label):\n",
        "                  cv2.imwrite(filename, output)\n",
        "                  imageSaved = 1\n",
        "                  break\n",
        "            key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "            # if the `q` key was pressed, break from the loop\n",
        "            if key == ord(\"q\"):\n",
        "                break\n",
        "        # release the file pointersq\n",
        "        print(\"[INFO] cleaning up...\")\n",
        "        writer.release()\n",
        "        vs.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5PNSwmTcYUTa"
      },
      "outputs": [],
      "source": [
        "V_path = \"/content/drive/MyDrive/WVM/Violence/Testingvideos/V_19.mp4\"\n",
        "NonV_path = \"/content/drive/MyDrive/WVM/Violence/Testingvideos/nonv.mp4\"\n",
        "V_test = \"/content/drive/MyDrive/WVM/Violence/Testingvideos/Testing video.mp4\"    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize minimum probability to filter weak detections along with\n",
        "# the threshold when applying non-maxima suppression\n",
        "MIN_CONF = 0.3\n",
        "NMS_THRESH = 0.3\n",
        "\n",
        "# define the minimum safe distance (in pixels) that two people can be\n",
        "# from each other\n",
        "MIN_DISTANCE = 50"
      ],
      "metadata": {
        "id": "t7AvHXWgaDXz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def detect_people(frame, net, ln, personIdx=0):\n",
        "\t# grab the dimensions of the frame and  initialize the list of\n",
        "\t# results\n",
        "\t(H, W) = frame.shape[:2]\n",
        "\tresults = []\n",
        "\n",
        "\t# construct a blob from the input frame and then perform a forward\n",
        "\t# pass of the YOLO object detector, giving us our bounding boxes\n",
        "\t# and associated probabilities\n",
        "\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
        "\t\tswapRB=True, crop=False)\n",
        "\tnet.setInput(blob)\n",
        "\tlayerOutputs = net.forward(ln)\n",
        "\n",
        "\t# initialize our lists of detected bounding boxes, centroids, and\n",
        "\t# confidences, respectively\n",
        "\tboxes = []\n",
        "\tcentroids = []\n",
        "\tconfidences = []\n",
        "\n",
        "\t# loop over each of the layer outputs\n",
        "\tfor output in layerOutputs:\n",
        "\t\t# loop over each of the detections\n",
        "\t\tfor detection in output:\n",
        "\t\t\t# extract the class ID and confidence (i.e., probability)\n",
        "\t\t\t# of the current object detection\n",
        "\t\t\tscores = detection[5:]\n",
        "\t\t\tclassID = np.argmax(scores)\n",
        "\t\t\tconfidence = scores[classID]\n",
        "\n",
        "\t\t\t# filter detections by (1) ensuring that the object\n",
        "\t\t\t# detected was a person and (2) that the minimum\n",
        "\t\t\t# confidence is met\n",
        "\t\t\tif classID == personIdx and confidence > MIN_CONF:\n",
        "\t\t\t\t# scale the bounding box coordinates back relative to\n",
        "\t\t\t\t# the size of the image, keeping in mind that YOLO\n",
        "\t\t\t\t# actually returns the center (x, y)-coordinates of\n",
        "\t\t\t\t# the bounding box followed by the boxes' width and\n",
        "\t\t\t\t# height\n",
        "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
        "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
        "\n",
        "\t\t\t\t# use the center (x, y)-coordinates to derive the top\n",
        "\t\t\t\t# and and left corner of the bounding box\n",
        "\t\t\t\tx = int(centerX - (width / 2))\n",
        "\t\t\t\ty = int(centerY - (height / 2))\n",
        "\n",
        "\t\t\t\t# update our list of bounding box coordinates,\n",
        "\t\t\t\t# centroids, and confidences\n",
        "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
        "\t\t\t\tcentroids.append((centerX, centerY))\n",
        "\t\t\t\tconfidences.append(float(confidence))\n",
        "\n",
        "\t# apply non-maxima suppression to suppress weak, overlapping\n",
        "\t# bounding boxes\n",
        "\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
        "\n",
        "\n",
        "\n",
        "\t# ensure at least one detection exists\n",
        "\tif len(idxs) > 0:\n",
        "\t\t# loop over the indexes we are keeping\n",
        "\t\tfor i in idxs.flatten():\n",
        "\t\t\t# extract the bounding box coordinates\n",
        "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
        "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
        "\n",
        "\t\t\t# update our results list to consist of the person\n",
        "\t\t\t# prediction probability, bounding box coordinates,\n",
        "\t\t\t# and the centroid\n",
        "\t\t\tr = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
        "\t\t\tresults.append(r)\n",
        "\n",
        "\t# return the list of results\n",
        "\treturn results"
      ],
      "metadata": {
        "id": "d3JJQldYaEfy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USAGE\n",
        "# python social_distance_detector.py --input pedestrians.mp4\n",
        "# python social_distance_detector.py --input pedestrians.mp4 --output output.avi\n",
        "\n",
        "# import the necessary packages\n",
        "from google.colab.patches import cv2_imshow\n",
        "from scipy.spatial import distance as dist\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def detect_mob():\n",
        "  # construct the argument parse and parse the arguments\n",
        "  ap = argparse.ArgumentParser()\n",
        "  ap.add_argument(\"-i\", \"--input\", type=str, default=\"\",\n",
        "      help=\"path to (optional) input video file\")\n",
        "  ap.add_argument(\"-o\", \"--output\", type=str, default=\"\",\n",
        "      help=\"path to (optional) output video file\")\n",
        "  ap.add_argument(\"-d\", \"--display\", type=int, default=1,\n",
        "      help=\"whether or not output frame should be displayed\")\n",
        "  args = vars(ap.parse_args([\"--input\",\"/content/drive/MyDrive/WVM/Mob/Mob-detection/Raw footage as Paris' Yellow Vest protest turns violent.mp4\",\"--output\",\"my_output.avi\",\"--display\",\"500\"]))\n",
        "\n",
        "  # load the COCO class labels our YOLO model was trained on\n",
        "  labelsPath = os.path.sep.join([\"/content/drive/MyDrive/WVM/Mob/Mob-detection/yolo-coco/coco.names\"])\n",
        "  LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
        "\n",
        "  # derive the paths to the YOLO weights and model configuration\n",
        "  weightsPath = os.path.sep.join([\"/content/drive/MyDrive/WVM/Mob/Mob-detection/yolo-coco/yolov3.weights\"])\n",
        "  configPath = os.path.sep.join([\"/content/drive/MyDrive/WVM/Mob/Mob-detection/yolo-coco/yolov3.cfg\"])\n",
        "\n",
        "  # load our YOLO object detector trained on COCO dataset (80 classes)\n",
        "  print(\"[INFO] loading YOLO from disk...\")\n",
        "  net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
        "\n",
        "  # determine only the *output* layer names that we need from YOLO\n",
        "  ln = net.getLayerNames()\n",
        "  ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "  # initialize the video stream and pointer to output video file\n",
        "  print(\"[INFO] accessing video stream...\")\n",
        "  vs = cv2.VideoCapture(args[\"input\"] if args[\"input\"] else 0)\n",
        "  writer = None\n",
        "  cnt=0\n",
        "  # loop over the frames from the video stream\n",
        "  while cnt<40:\n",
        "      # read the next frame from the file\n",
        "      (grabbed, frame) = vs.read()\n",
        "      cnt=cnt+1\n",
        "      print(cnt)\n",
        "\n",
        "      # if the frame was not grabbed, then we have reached the end\n",
        "      # of the stream\n",
        "      if not grabbed:\n",
        "          break\n",
        "\n",
        "      # resize the frame and then detect people (and only people) in it\n",
        "      frame = imutils.resize(frame, width=700)\n",
        "      results = detect_people(frame, net, ln,\n",
        "      personIdx=LABELS.index(\"person\"))                                                                                                  \n",
        "\n",
        "      # initialize the set of indexes that violate the minimum social\n",
        "      # distance\n",
        "      violate = set()\n",
        "\n",
        "      # ensure there are *at least* five people detections (required in\n",
        "      # order to compute our pairwise distance maps)\n",
        "      if len(results) >= 1:\n",
        "          # extract all centroids from the results and compute the\n",
        "          # Euclidean distances between all pairs of the centroids\n",
        "          centroids = np.array([r[2] for r in results])\n",
        "          D = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
        "\n",
        "          # loop over the upper triangular of the distance matrix\n",
        "          for i in range(0, D.shape[0]):\n",
        "              for j in range(i + 1, D.shape[1]):\n",
        "                  # check to see if the distance between any two\n",
        "                  # centroid pairs is less than the configured number\n",
        "                  # of pixels\n",
        "                  if D[i, j] < MIN_DISTANCE:\n",
        "                      # update our violation set with the indexes of\n",
        "                      # the centroid pairs\n",
        "                      violate.add(i)\n",
        "                      violate.add(j)\n",
        "\n",
        "      # loop over the results\n",
        "          for (i, (prob, bbox, centroid)) in enumerate(results):\n",
        "              # extract the bounding box and centroid coordinates, then\n",
        "              # initialize the color of the annotation\n",
        "              (startX, startY, endX, endY) = bbox\n",
        "              (cX, cY) = centroid\n",
        "              color = (0, 255, 0)\n",
        "\n",
        "              # if the index pair exists within the violation set, then\n",
        "              # update the color\n",
        "              if i in violate:\n",
        "                  color = (0, 0, 255)\n",
        "\n",
        "              # draw (1) a bounding box around the person and (2) the\n",
        "              # centroid coordinates of the person,\n",
        "              cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "              cv2.circle(frame, (cX, cY), 5, color, 1)\n",
        "\n",
        "      # draw the total number of social distancing violations on the\n",
        "      # output frame\n",
        "      text = \"Mob Detected: {}\".format(len(violate))\n",
        "      cv2.putText(frame, text, (10, frame.shape[0] - 25),\n",
        "          cv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)\n",
        "\n",
        "      key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "          # if the `q` key was pressed, break from the loop\n",
        "      if key == ord(\"q\"):\n",
        "          break\n",
        "\n",
        "      # if an output video file path has been supplied and the video\n",
        "      # writer has not been initialized, do so now\n",
        "      if args[\"output\"] != \"\" and writer is None:\n",
        "          # initialize our video writer\n",
        "          push_mob()\n",
        "          fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "          writer = cv2.VideoWriter(args[\"output\"], fourcc, 25,\n",
        "              (frame.shape[1], frame.shape[0]), True)\n",
        "\n",
        "      # if the video writer is not None, write the frame to the output\n",
        "      # video file\n",
        "      if writer is not None:\n",
        "          writer.write(frame)"
      ],
      "metadata": {
        "id": "4EbwY70laJQi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "PcAr0PLYaM9C"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_weapons():\n",
        "  model = load_model('/content/drive/MyDrive/WVM/Weapons/MV1_Edge_Augment.h5')\n",
        "  with open('/content/drive/MyDrive/WVM/Weapons/labels.txt', 'r') as f:\n",
        "      labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "  # Define the input image size\n",
        "  IMG_SIZE = 224\n",
        "\n",
        "  # Define the function to preprocess the input image\n",
        "  def preprocess_image(image):\n",
        "      img = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "      img = img / 255.0\n",
        "      img = np.expand_dims(img, axis=0)\n",
        "      return img\n",
        "\n",
        "  # Define the function to perform object detection and classification on each frame of the video\n",
        "  def detect_and_classify_objects(video_path):\n",
        "      # Open the video file\n",
        "      writer = None\n",
        "      ap = argparse.ArgumentParser()\n",
        "      ap.add_argument(\"-i\", \"--input\", type=str, default=\"\",\n",
        "      help=\"path to (optional) input video file\")\n",
        "      ap.add_argument(\"-o\", \"--output\", type=str, default=\"\",\n",
        "      help=\"path to (optional) output video file\")\n",
        "      ap.add_argument(\"-d\", \"--display\", type=int, default=1,\n",
        "      help=\"whether or not output frame should be displayed\")\n",
        "      cap = cv2.VideoCapture(video_path)\n",
        "      # Loop through each frame of the video\n",
        "      cnt=0\n",
        "      args = vars(ap.parse_args([\"--input\",'/content/drive/MyDrive/WVM/Weapons/Model 59 Smith & Wesson 9mm.mp4',\"--output\",\"weaponsout.avi\",\"--display\",\"500\"]))\n",
        "      while True:\n",
        "          # Read the frame from the video\n",
        "          ret, frame = cap.read()\n",
        "          cnt=cnt+1\n",
        "          if not ret:\n",
        "              break\n",
        "          # Preprocess the input image\n",
        "          image = preprocess_image(frame)\n",
        "          # Perform object detection and classification\n",
        "          predictions = model.predict(image)\n",
        "          class_index = np.argmax(predictions)\n",
        "          class_label = labels[class_index]\n",
        "          confidence = predictions[0][class_index]\n",
        "          # Draw the class label and confidence on the frame\n",
        "          text = '{} ({:.2f}%)'.format(class_label, confidence*100)\n",
        "          cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "          if args[\"output\"] != \"\" and writer is None:\n",
        "          # initialize our video writer\n",
        "            push_wep()\n",
        "            fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "            writer = cv2.VideoWriter(args[\"output\"], fourcc, 25,\n",
        "                (frame.shape[1], frame.shape[0]), True)\n",
        "\n",
        "      # if the video writer is not None, write the frame to the output\n",
        "      # video file\n",
        "          if writer is not None:\n",
        "            writer.write(frame)\n",
        "          # Wait for a key press to exit\n",
        "          if cv2.waitKey(1) & cnt == 40:\n",
        "              break\n",
        "      # Release the video file and destroy the window\n",
        "      cap.release()\n",
        "      cv2.destroyAllWindows()\n",
        "\n",
        "  # Test the function on an example video\n",
        "  video_path = '/content/drive/MyDrive/WVM/Weapons/Model 59 Smith & Wesson 9mm.mp4'\n",
        "  detect_and_classify_objects(video_path)"
      ],
      "metadata": {
        "id": "NWt6F4ZdaPka"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing as mp"
      ],
      "metadata": {
        "id": "-V22M_j8aVwT"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "functions=[detectViolence(V_path), detect_mob(), detect_weapons()]\n",
        "processes=[]\n",
        "for functions in functions:\n",
        "    process = mp.Process(target=functions)\n",
        "    processes.append(process)\n",
        "    process.start()\n",
        "\n",
        "# Wait for each process to complete\n",
        "for process in processes:\n",
        "    process.join()\n",
        "\n",
        "print('All functions have completed.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwGYzn4RaWH7",
        "outputId": "d14f2603-e005-4f09-e91b-742dcafb2d86"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model ...\n",
            "1/1 [==============================] - 1s 889ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "[INFO] cleaning up...\n",
            "[INFO] loading YOLO from disk...\n",
            "[INFO] accessing video stream...\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "1/1 [==============================] - 1s 839ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "All functions have completed.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}